[
    {
        "doi": "arXiv:2510.00083",
        "title": "Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks",
        "author": "Hanjiang Hu, Bowei Li, Ziwei Wang, Tianhao Wei, Casidhe Hutchison, Eric Sample, Changliu Liu",
        "date": "2025-09-30",
        "abstract": "Deep neural networks have been widely adopted in many vision and robotics applications with visual inputs. It is essential to verify its robustness against semantic transformation perturbations, such as brightness and contrast. However, current certified training and robustness certification methods face the challenge of over-parameterization, which hinders the tightness and scalability due to the over-complicated neural networks. To this end, we first analyze stability and variance of layers and neurons against input perturbation, showing that certifiable robustness can be indicated by a fundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce a novel neural network pruning method that removes neurons with low USN and retains those with high USN, thereby preserving model expressiveness without over-parameterization. To further enhance this pruning process, we propose a new Wasserstein distance loss to ensure that pruned neurons are more concentrated across layers. We validate our approach through extensive experiments on the challenging robust keypoint detection task, which involves realistic brightness and contrast perturbations, demonstrating that our method achieves superior robustness certification performance and efficiency compared to baselines.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00459",
        "title": "Stochastic Representation of Time-Evolving Neural Network-based Wavefunctions",
        "author": "Bizi Huang, Weizhong Fu, Ji Chen",
        "date": "2025-10-01",
        "abstract": "Solving the time-dependent Schr\\\"odinger equation (TDSE) is pivotal for modeling non-adiabatic electron dynamics, a key process in ultrafast spectroscopy and laser-matter interactions. However, exact solutions to the TDSE remain computationally prohibitive for most realistic systems, as the Hilbert space expands exponentially with dimensionality. In this work, we propose an approach integrating the stochastic representation framework with a neural network wavefunction ansatz, a flexible model capable of approximating time-evolving quantum wavefunctions. We first validate the method on one-dimensional single-electron systems, focusing on ionization dynamics under intense laser fields, a critical process in attosecond physics. Our results demonstrate that the approach accurately reproduces key features of quantum evolution, including the energy and dipole evolution during ionization. We further show the feasibility of extending this approach to three-dimensional systems. Due to the increased complexity of real-time simulations in higher dimensions, these results remain at an early stage and highlight the need for more advanced stabilization strategies.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00442",
        "title": "Randomized Matrix Sketching for Neural Network Training and Gradient Monitoring",
        "author": "Harbir Antil, Deepanshu Verma",
        "date": "2025-10-01",
        "abstract": "Neural network training relies on gradient computation through backpropagation, yet memory requirements for storing layer activations present significant scalability challenges. We present the first adaptation of control-theoretic matrix sketching to neural network layer activations, enabling memory-efficient gradient reconstruction in backpropagation. This work builds on recent matrix sketching frameworks for dynamic optimization problems, where similar state trajectory storage challenges motivate sketching techniques. Our approach sketches layer activations using three complementary sketch matrices maintained through exponential moving averages (EMA) with adaptive rank adjustment, automatically balancing memory efficiency against approximation quality. Empirical evaluation on MNIST, CIFAR-10, and physics-informed neural networks demonstrates a controllable accuracy-memory tradeoff. We demonstrate a gradient monitoring application on MNIST showing how sketched activations enable real-time gradient norm tracking with minimal memory overhead. These results establish that sketched activation storage provides a viable path toward memory-efficient neural network training and analysis.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.03994",
        "title": "Optimal estimation of a factorizable density using diffusion models with ReLU neural networks",
        "author": "Jianqing Fan, Yihong Gu, Ximing Li",
        "date": "2025-10-05",
        "abstract": "This paper investigates the score-based diffusion models for density estimation when the target density admits a factorizable low-dimensional nonparametric structure. To be specific, we show that when the log density admits a $d^*$-way interaction model with $\\beta$-smooth components, the vanilla diffusion model, which uses a fully connected ReLU neural network for score matching, can attain optimal $n^{-\\beta/(2\\beta+d^*)}$ statistical rate of convergence in total variation distance. This is, to the best of our knowledge, the first in the literature showing that diffusion models with standard configurations can adapt to the low-dimensional factorizable structures. The main challenge is that the low-dimensional factorizable structure no longer holds for most of the diffused timesteps, and it is very challenging to show that these diffused score functions can be well approximated without a significant increase in the number of network parameters. Our key insight is to demonstrate that the diffused score functions can be decomposed into a composition of either super-smooth or low-dimensional components, leading to a new approximation error analysis of ReLU neural networks with respect to the diffused score function. The rate of convergence under the 1-Wasserstein distance is also derived with a slight modification of the method.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.03996",
        "title": "FHEON: A Configurable Framework for Developing Privacy-Preserving Neural Networks Using Homomorphic Encryption",
        "author": "Nges Brian Njungle, Eric Jahns, Michel A. Kinsy",
        "date": "2025-10-05",
        "abstract": "The widespread adoption of Machine Learning as a Service raises critical privacy and security concerns, particularly about data confidentiality and trust in both cloud providers and the machine learning models. Homomorphic Encryption (HE) has emerged as a promising solution to this problems, allowing computations on encrypted data without decryption. Despite its potential, existing approaches to integrate HE into neural networks are often limited to specific architectures, leaving a wide gap in providing a framework for easy development of HE-friendly privacy-preserving neural network models similar to what we have in the broader field of machine learning. In this paper, we present FHEON, a configurable framework for developing privacy-preserving convolutional neural network (CNN) models for inference using HE. FHEON introduces optimized and configurable implementations of privacy-preserving CNN layers including convolutional layers, average pooling layers, ReLU activation functions, and fully connected layers. These layers are configured using parameters like input channels, output channels, kernel size, stride, and padding to support arbitrary CNN architectures. We assess the performance of FHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16, ResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within +/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models. Notably, on a consumer-grade CPU, the models build on FHEON achieved 98.5% accuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2% accuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20. Additionally, FHEON operates within a practical memory budget requiring not more than 42.3 GB for VGG-16.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.03923",
        "title": "On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks",
        "author": "Mingsong Yan, Charles Kulick, Sui Tang",
        "date": "2025-10-04",
        "abstract": "Continuous-depth graph neural networks, also known as Graph Neural Differential Equations (GNDEs), combine the structural inductive bias of Graph Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs, offering a scalable and principled framework for modeling dynamics on graphs. In this paper, we present a rigorous convergence analysis of GNDEs with time-varying parameters in the infinite-node limit, providing theoretical insights into their size transferability. To this end, we introduce Graphon Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of GNDEs and establish their well-posedness. Leveraging tools from graphon theory and dynamical systems, we prove the trajectory-wise convergence of GNDE solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence rates under two deterministic graph sampling regimes: (1) weighted graphs sampled from smooth graphons, and (2) unweighted graphs sampled from $\\{0,1\\}$-valued (discontinuous) graphons. We further establish size transferability bounds, providing theoretical justification for the practical strategy of transferring GNDE models trained on moderate-sized graphs to larger, structurally similar graphs without retraining. Numerical experiments using synthetic and real data support our theoretical findings.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.03648",
        "title": "SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network",
        "author": "Huijing Zhang, Muyang Cao, Linshan Jiang, Xin Du, Di Yu, Changze Lv, Shuiguang Deng",
        "date": "2025-10-04",
        "abstract": "Continuous learning of novel classes is crucial for edge devices to preserve data privacy and maintain reliable performance in dynamic environments. However, the scenario becomes particularly challenging when data samples are insufficient, requiring on-device few-shot class-incremental learning (FSCIL) to maintain consistent model performance. Although existing work has explored parameter-efficient FSCIL frameworks based on artificial neural networks (ANNs), their deployment is still fundamentally constrained by limited device resources. Inspired by neural mechanisms, Spiking neural networks (SNNs) process spatiotemporal information efficiently, offering lower energy consumption, greater biological plausibility, and compatibility with neuromorphic hardware than ANNs. In this work, we present an SNN-based method for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We first propose sparsity-conditioned neuronal dynamics, in which most neurons remain stable while a subset stays active, thereby mitigating catastrophic forgetting. To further cope with spike non-differentiability in gradient estimation, we employ zeroth-order optimization. Moreover, during incremental learning sessions, we enhance the discriminability of new classes through subspace projection, which alleviates overfitting to novel classes. Extensive experiments conducted on two standard benchmark datasets (CIFAR100 and Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture, and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods, specifically achieving at least 4.01% improvement at the last incremental session on Mini-ImageNet and 20% lower energy cost over baseline methods with practical implementation.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.01263",
        "title": "Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency",
        "author": "Yaron Meirovitch, Fuming Yang, Jeff Lichtman, Nir Shavit",
        "date": "2025-09-26",
        "abstract": "Most pruning methods remove parameters ranked by impact on loss (e.g., magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each unit a local traffic budget (the product of its long-term on-rate $a_i$ and fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding entropy under a global traffic budget yields a selectivity-audience balance, $\\log\\frac{1-a_i}{a_i}=\\beta k_i$. BB enforces this balance with simple local actuators that prune either fan-in (to lower activity) or fan-out (to reduce broadcast). In practice, BB increases coding entropy and decorrelation and improves accuracy at matched sparsity across Transformers for ASR, ResNets for face identification, and 3D U-Nets for synapse prediction, sometimes exceeding dense baselines. On electron microscopy images, it attains state-of-the-art F1 and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests a path toward learning more diverse and efficient representations.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00133",
        "title": "Large Language Models Inference Engines based on Spiking Neural Networks",
        "author": "Adarsha Balaji, Sandeep Madireddy",
        "date": "2025-09-30",
        "abstract": "Foundational models based on the transformer architecture are currently the state-of-the-art in general language modeling, as well as in scientific areas such as material science and climate. However, training and deploying these models is computationally challenging as the time and space complexity has a quadratic relation to the input sequence length. Several efforts exploring efficient computational paradigms and model architectures to address these limitations have been made. In this work, we explore spiking neural networks (SNNs) to design transformer models. A challenge in training large-scale SNNs, using existing surrogate learning methods is inefficient and time-consuming. On the other hand, techniques to convert existing transformer-based models to their SNN equivalent are not scalable, as achieving optimal performance comes at the cost of a large number of spike time-steps, i.e. increased latency. To address this, we propose NeurTransformer, a methodology for designing transformer-based SNN for inference using a supervised fine-tuning approach with existing conversion methods. The proposed methodology works by: (1) replacing the self-attention mechanism with a spike-based self-attention (SSA), (2) converting the feed-forward block of the trained transformer model to its equivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate learning algorithms. We benchmark the proposed methodology and demonstrate its accuracy and scalability using three variants of the GPT-2 model of increasing model size. We observe that the converted GPT-2 small models demonstrate a 5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we demonstrate the energy efficiency of the SSA block compared to the ASA block and show between 64.71% and 85.28% reductions in estimated energy consumption when implementing the self-attention mechanism on a digital hardware.",
        "journal": "N/A"
    },
    {
        "doi": "10.1103/fqxr-r8vw",
        "title": "Improving neural network performance for solving quantum sign structure",
        "author": "Xiaowei Ou, Tianshu Huang, Vidvuds Ozolins",
        "date": "2025-10-02",
        "abstract": "Neural quantum states have emerged as a widely used approach to the numerical study of the ground states of non-stoquastic Hamiltonians. However, existing approaches often rely on a priori knowledge of the sign structure or require a separately pre-trained phase network. We introduce a modified stochastic reconfiguration method that effectively uses differing imaginary time steps to evolve the amplitude and phase. Using a larger time step for phase optimization, this method enables a simultaneous and efficient training of phase and amplitude neural networks. The efficacy of our method is demonstrated on the Heisenberg J_1-J_2 model.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00711",
        "title": "An InAsSb surface quantum well with in-situ deposited Nb as a platform for semiconductor-superconductor hybrid devices",
        "author": "Sjoerd Telkamp, Zijin Lei, Tommaso Antonelli, Christian Reichl, Ilya Besedin, Georg Jakobs, Stefan Fält, Christian Marty, Rüdiger Schott, Werner Wegscheider",
        "date": "2025-10-01",
        "abstract": "We present a novel semiconductor-superconductor hybrid material based on a molecular beam epitaxially grown InAsSb surface quantum well with an in-situ deposited Nb top layer. Relative to conventional Al-InAs based systems, the InAsSb surface quantum well offers a lower effective mass and stronger spin-orbit interaction, while the Nb layer has a higher critical temperature and a larger critical magnetic field. The in-situ deposition of the Nb results in a high-quality interface that enables strong coupling to the InAsSb quantum well. Transport measurements on Josephson junctions reveal an induced superconducting gap of 1.3 meV. Furthermore, a planar asymmetric SQUID is realized, exhibiting gate-tunable superimposed oscillations originating from both the individual Josephson junction and the full SQUID loop. The large induced superconducting gap combined with strong spin-orbit interaction position this material as an attractive platform for experiments exploring gate-tunable superconductivity and topological superconducting devices.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.02794",
        "title": "Using Landau quantization to probe disorder in semiconductor heterostructures",
        "author": "Asser Elsayed, Davide Costa, Lucas E. A. Stehouwer, Alberto Tosato, Mario Lodari, Brian Paquelet Wuetz, Davide Degli Esposti, Giordano Scappucci",
        "date": "2025-10-03",
        "abstract": "Understanding scattering mechanisms in semiconductor heterostructures is crucial to reducing sources of disorder and ensuring high yield and uniformity in large spin qubit arrays. Disorder of the parent two-dimensional electron or hole gas is commonly estimated by the critical, percolation-driven density associated with the metal-insulator transition. However, a reliable estimation of the critical density within percolation theory is hindered by the need to measure conductivity with high precision at low carrier densities, where experiments are most difficult. Here, we connect experimentally percolation density and quantum Hall plateau width, in line with an earlier heuristic intuition, and offer an alternative method for characterizing semiconductor heterostructure disorder.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.04789",
        "title": "All-optical nonlinear phase modulation in open semiconductor microcavities",
        "author": "Fedor A. Benimetskiy, Paul M. Walker, Anthony Ellul, Oleksandr Kyriienko, Martina Morassi, Aristide Lemaître, Tommi Isoniemi, Maurice S. Skolnick, Jacqueline Bloch, Sylvain Ravets, Dmitry N. Krizhanovskii",
        "date": "2025-10-06",
        "abstract": "We report a significant advancement in ultra low power light-by-light phase modulation using open semiconductor microcavities in the strong light-matter coupling regime. We achieve cross-phase modulation of up to 247$\\pm$17 mrad per particle between laser beams attenuated to single-photon average intensities. This breakthrough extends the potential for quantum information processing and nonlinear quantum optics in strongly coupled light-matter systems, setting a new benchmark in the field without relying on atom-like emitters. Our findings suggest promising new avenues for scalable quantum optical technologies.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00305",
        "title": "Gate-tunable Josephson parametric amplifiers based on semiconductor nanowires",
        "author": "Raphael Rousset-Zenou, Nicolas Aparicio, Simon Messelot, Rasmus D. Schlosser, Martin Bjergfelt, Julien Renard, Moïra Hocevar, Jesper Nygård",
        "date": "2025-09-30",
        "abstract": "Superconductor-semiconductor hybrid materials have been extensively used for experiments on electrically tunable quantum devices. Notably, Josephson junctions utilizing nanowire weak links have enabled a number of new gate-tunable qubits, including gatemons, Andreev level qubits and spin qubits. Conversely, superconducting parametric amplifiers based on Josephson junctions have not yet been implemented using nanowires, even though such nearly quantum limited amplifiers are key elements in experiments on quantum circuits. Here we present Josephson parametric amplifiers based on arrays of parallel InAs nanowires that feature a large critical current as required for linear amplification. The resonance frequency of the devices is gate-tunable by almost 1 GHz, with a gain exceeding 20 dB in multiple frequencies and noise approaching the quantum-limit. This new platform enables on-chip integration of gate-tunable qubits with quantum limited amplifiers using the same hybrid materials and on any substrate.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.04462",
        "title": "Robust iSWAP gates for semiconductor spin qubits with local driving",
        "author": "Qi-Pei Liu, Zheng-Yuan Xue",
        "date": "2025-10-06",
        "abstract": "Scalable quantum computation demands high-fidelity two-qubit gates. However, decoherence and control errors are inevitable, which can decrease the quality of implemented quantum operations. We propose a robust iSWAP gate protocol for semiconductor spin qubits, which is a promising platform for scalable quantum computing. Our scheme uses only local microwave drives on conventional exchange-coupled spin qubits. This approach simultaneously addresses two critical challenges on semiconductor quantum computing: it suppresses low-frequency noise via continuous dynamical decoupling, and it circumvents the control difficulties associated with the ac modulation of the exchange interaction. We further develop a composite pulse sequence to remove drive-strength constraints and a dynamically corrected method to provide first-order immunity to microwave amplitude errors.Numerical simulations confirm that our scheme can achieve fidelity above the fault-tolerance threshold under current experimental conditions, offering a building block for practical quantum processors.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00865",
        "title": "Determining the Density of In-gap States in Organic Semiconductors: A Pitfall of Photoelectron Yield Spectroscopy",
        "author": "Ryotaro Nakazawa, Masaya Kitaoka, Ryota Kaimori, Manato Tateno, Runa Hoshikawa, Yuya Tanaka, Hisao Ishii",
        "date": "2025-10-01",
        "abstract": "Accurate determination of electronic states with low density in the bandgap (in-gap states) is crucial for understanding and optimizing the performance of organic optoelectronic devices. In the device research field, photoelectron yield spectroscopy (PYS) has been widely used to evaluate such states, and its derivative is commonly employed to estimate the density of states (DOS). However, low-energy photons in PYS can generate excitons and anions in organic semiconductors, raising questions about whether derivative PYS spectra truly represent the DOS. Here, we applied hv-dependent high-sensitivity ultraviolet photoelectron spectroscopy with low-energy photons to probe the origin of photoelectrons in organic semiconductors. We show that PYS signals arise from the single-quantum external photoelectron effect (SQEPE) of in-gap states, from SQEPE of the singly occupied molecular orbital (SOMO) in anions, and biphotonic electron emission (BEE) effect via exciton fusions. Because the BEE signal masks the DOS, derivative PYS can misestimate the DOS of in-gap states. In contrast, constant final state yield spectroscopy (CFS-YS) can reliably determine the DOS. For example, in Tris(8-hydroxyquinoline) aluminum (Alq3), we observed BEE, and CFS-YS revealed the DOS of in-gap states and SOMO over six orders of magnitude. The direct determination of the SOMO DOS allowed us to clarify the role of Alq3 in the electron injection layer in organic light-emitting diodes. Moreover, the BEE process can act as both a carrier-generation and degradation pathway in organic optoelectronic devices. These findings demonstrate that CFS-YS provides a reliable method to determine the DOS of in-gap states and to probe exciton and/or anion interactions. We establish practical guidelines for determining the DOS of in-gap states for low-energy photon measurements, such as examining photon-flux dependence.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.03575",
        "title": "High-spin magnetic ground states of neutral dopant clusters in semiconductors",
        "author": "Rhine Samajdar, Haonan Zhou, R. N. Bhatt",
        "date": "2025-10-03",
        "abstract": "High-spin states hold significant promise for classical and quantum information storage and emerging magnetic memory technologies. Here, we present a systematic framework for engineering such high-spin magnetic states in dopant clusters formed from substitutional impurities in semiconductors. In single-valley materials such as gallium arsenide, impurity states are hydrogenic and exchange interactions generally favor low-spin configurations, except in special geometries. In contrast, multivalley semiconductors exhibit oscillatory form factors in their exchange couplings, enabling the controlled suppression of selected hopping processes and exchange couplings. Exploiting this feature, we demonstrate how carefully arranged impurities in aluminum arsenide, germanium, and silicon can stabilize ground states with a net spin that scale extensively with system size. Within effective mass theory and the tight-binding approximation for hopping, we construct explicit examples ranging from finite clusters to extended lattices and fractal-like tilings. In two dimensions, we identify several favorable dopant geometries supporting a net spin equal to around half of the fully polarized value in the thermodynamic limit, including one which achieves over $70\\%$ polarization. Our results provide a general design principle for harnessing valley degeneracy in semiconductors to construct robust high-spin states and outline a pathway for their experimental realization via precision implantation of dopants.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.01921",
        "title": "Electrically tunable ultrafast dynamics and interactions of hybrid excitons in a 2D semiconductor bilayer",
        "author": "Edoardo Lopriore, Charalambos Louca, Armando Genco, Irantzu Landa, Daniel Erkensten, Charles J. Sayers, Samuel Brem, Raul Perea-Causin, Kenji Watanabe, Takashi Taniguchi, Christoph Gadermaier, Ermin Malic, Giulio Cerullo, Stefano Dal Conte, Andras Kis",
        "date": "2025-10-02",
        "abstract": "Extended efforts have been devoted to the study of strongly-interacting excitons and their dynamics, towards macroscopic quantum states of matter such as Bose-Einstein condensates of excitons and polaritons. Momentum-direct layer-hybridized excitons in transition metal dichalcogenides have attracted considerable attention due to their high oscillator strength and dipolar nature. However, the tunability of their interactions and dynamics remains unexplored. Here, we achieve an unprecedented control over the nonlinear properties of dipolar layer-hybridized excitons in an electrically gated van der Waals homobilayer monitored by transient optical spectroscopy. By applying a vertical electric field, we reveal strong Coulomb interactions of dipolar hybrid excitons, leading to opposite density-dependent energy shifts of the two main hybrid species based on their dipolar orientation, together with a strongly enhanced optical saturation of their absorption. Furthermore, by electrically tuning the interlayer tunneling between the hybridized carriers, we significantly extend the formation time of hybrid excitons, while simultaneously increasing their decay times. Our findings have implications for the search on quantum blockade and condensation of excitons and dipolaritons in two-dimensional materials.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.03052",
        "title": "Ultrafast dynamics of coherent exciton-polaritons in van der Waals semiconductor metasurfaces",
        "author": "Luca Sortino, Armando Genco, Cristina Cruciano, Michele Guizzardi, Daniel Timmer, Thomas Weber, Jonathan O. Tollerud, Francesco Gucci, Matteo Corti, Gianluca Valentini, Cristian Manzoni, Stefano Dal Conte, Christoph Lienau, Jeffrey A. Davis, Stefan A. Maier, Andreas Tittl, Giulio Cerullo",
        "date": "2025-10-03",
        "abstract": "Enabling coherent light-matter interactions is a critical step toward next-generation quantum technologies. However, achieving this under ambient temperature conditions remains challenging due to rapid dephasing in optically excited systems. Optical metasurfaces based on quasi-bound states in the continuum have recently emerged as a powerful platform for reaching the strong light-matter coupling regime in flat, subwavelength thickness devices. Here, we investigate ultrafast exciton-polariton dynamics in self-hybridized WS$_2$ thin-film metasurfaces. Using hyperspectral momentum-resolved imaging, we reconstruct the highly anisotropic exciton-polariton dispersion, with a transition from positive to negative effective mass along orthogonal symmetry axes. Femtosecond pump-probe and multidimensional spectroscopy reveal detuning-dependent polariton dynamics with a coherence time up to ~110 fs, and allow direct observation of the coherent dynamics through ultrafast Rabi oscillations with ~45 fs period. We describe this behaviour with a three-eigenstate model that couples the photonic resonance with both bright and dark excitons, extending the conventional two-state picture of strong coupling. Our results establish van der Waals metasurfaces as a promising platform for next-generation polaritonic devices, enabling coherent quantum transfer of matter excitations at room temperature.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2509.00453",
        "title": "Probing the Nanoscale Excitonic Landscape and Quantum Confinement of Excitons in Gated Monolayer Semiconductors",
        "author": "Yueh-Chun Wu, Bogdan Dryzhakov, Huan Zhao, Ivan Vlassiouk, Kyle Kelley, Takashi Taniguchi, Kenji Watanabe, Jun Yan, Benjamin Lawrie",
        "date": "2025-08-30",
        "abstract": "Engineering and probing excitonic properties at the nanoscale remains a central challenge in quantum photonics and optoelectronics. While exciton confinement via electrical control and strain engineering has been demonstrated in 2D semiconductors, substantial nanoscale heterogeneity limits the scalability of 2D quantum photonic device architectures. In this work, we use cathodoluminescence spectroscopy to probe the excitonic landscape of monolayer $WS_2$ under electrostatic gating. Exploiting the high spatial resolution of the converged electron beam, we resolve a homojunction arising between gated and ungated regions. Moreover, we reveal an exciton confinement channel arising from an unconventional doping mechanism driven by the interplay between the electron beam and the applied gate fields. These findings offer new insights into the optoelectronic behavior of monolayer semiconductors under the combined influence of electron-beam excitation and electrostatic gating. Our approach provides a pathway for exciton manipulation at the nanoscale and opens opportunities for controlling quantum-confined exciton transport in two-dimensional materials.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.04927",
        "title": "Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data",
        "author": "Usman Akram, Yiyue Chen, Haris Vikalo",
        "date": "2025-10-06",
        "abstract": "Training automatic modulation classification (AMC) models on centrally aggregated data raises privacy concerns, incurs communication overhead, and often fails to confer robustness to channel shifts. Federated learning (FL) avoids central aggregation by training on distributed clients but remains sensitive to class imbalance, non-IID client distributions, and limited labeled samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with triplet-loss self-supervision on unlabeled I/Q sequences across clients, followed by per-client SVMs on small labeled sets. We establish convergence of the federated representation learning procedure and a separability guarantee for the downstream classifier under feature noise. Experiments on synthetic and over-the-air datasets show consistent gains over supervised FL baselines under heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.04908",
        "title": "How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning",
        "author": "Haotian Gao, Zheng Dong, Jiawei Yong, Shintaro Fukushima, Kenjiro Taura, Renhe Jiang",
        "date": "2025-10-06",
        "abstract": "Spatio-temporal forecasting is essential for real-world applications such as traffic management and urban computing. Although recent methods have shown improved accuracy, they often fail to account for dynamic deviations between current inputs and historical patterns. These deviations contain critical signals that can significantly affect model performance. To fill this gap, we propose ST-SSDL, a Spatio-Temporal time series forecasting framework that incorporates a Self-Supervised Deviation Learning scheme to capture and utilize such deviations. ST-SSDL anchors each input to its historical average and discretizes the latent space using learnable prototypes that represent typical spatio-temporal patterns. Two auxiliary objectives are proposed to refine this structure: a contrastive loss that enhances inter-prototype discriminability and a deviation loss that regularizes the distance consistency between input representations and corresponding prototypes to quantify deviation. Optimized jointly with the forecasting objective, these components guide the model to organize its hidden space and improve generalization across diverse input conditions. Experiments on six benchmark datasets show that ST-SSDL consistently outperforms state-of-the-art baselines across multiple metrics. Visualizations further demonstrate its ability to adaptively respond to varying levels of deviation in complex spatio-temporal scenarios. Our code and datasets are available at https://github.com/Jimmy-7664/ST-SSDL.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.04454",
        "title": "Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners",
        "author": "Xiangchi Yuan, Xiang Chen, Tong Yu, Dachuan Shi, Can Jin, Wenke Lee, Saayan Mitra",
        "date": "2025-10-06",
        "abstract": "Large Language Models (LLMs) show strong reasoning abilities, often amplified by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although RL algorithms can substantially improve reasoning, they struggle to expand reasoning boundaries because they learn from their own reasoning trajectories rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers complementary benefits but typically requires large-scale data and risks overfitting. Recent attempts to combine SFT and RL face three main challenges: data inefficiency, algorithm-specific designs, and catastrophic forgetting. We propose a plug-and-play framework that dynamically integrates SFT into RL by selecting challenging examples for SFT. This approach reduces SFT data requirements and remains agnostic to the choice of RL or SFT algorithm. To mitigate catastrophic forgetting of RL-acquired skills during SFT, we select high-entropy tokens for loss calculation and freeze parameters identified as critical for RL. Our method achieves state-of-the-art (SoTA) reasoning performance using only 1.5% of the SFT data and 20.4% of the RL data used by prior SoTA, providing an efficient and plug-and-play solution for combining SFT and RL in reasoning post-training.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.03993",
        "title": "Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning",
        "author": "Yaxin Hou, Bo Han, Yuheng Jia, Hui Liu, Junhui Hou",
        "date": "2025-10-05",
        "abstract": "Current long-tailed semi-supervised learning methods assume that labeled data exhibit a long-tailed distribution, and unlabeled data adhere to a typical predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed). However, the distribution of the unlabeled data is generally unknown and may follow an arbitrary distribution. To tackle this challenge, we propose a Controllable Pseudo-label Generation (CPG) framework, expanding the labeled dataset with the progressively identified reliable pseudo-labels from the unlabeled dataset and training the model on the updated labeled dataset with a known distribution, making it unaffected by the unlabeled data distribution. Specifically, CPG operates through a controllable self-reinforcing optimization cycle: (i) at each training step, our dynamic controllable filtering mechanism selectively incorporates reliable pseudo-labels from the unlabeled dataset into the labeled dataset, ensuring that the updated labeled dataset follows a known distribution; (ii) we then construct a Bayes-optimal classifier using logit adjustment based on the updated labeled data distribution; (iii) this improved classifier subsequently helps identify more reliable pseudo-labels in the next training step. We further theoretically prove that this optimization cycle can significantly reduce the generalization error under some conditions. Additionally, we propose a class-aware adaptive augmentation module to further improve the representation of minority classes, and an auxiliary branch to maximize data utilization by leveraging all labeled and unlabeled samples. Comprehensive evaluations on various commonly used benchmark datasets show that CPG achieves consistent improvements, surpassing state-of-the-art methods by up to \\textbf{15.97\\%} in accuracy. The code is available at https://github.com/yaxinhou/CPG.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.01248",
        "title": "SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs",
        "author": "Ruyue Liu, Rong Yin, Xiangzhen Bo, Xiaoshuai Hao, Yong Liu, Jinwen Zhong, Can Ma, Weiping Wang",
        "date": "2025-09-24",
        "abstract": "Large scale pretrained models have revolutionized Natural Language Processing (NLP) and Computer Vision (CV), showcasing remarkable cross domain generalization abilities. However, in graph learning, models are typically trained on individual graph datasets, limiting their capacity to transfer knowledge across different graphs and tasks. This approach also heavily relies on large volumes of annotated data, which presents a significant challenge in resource-constrained settings. Unlike NLP and CV, graph structured data presents unique challenges due to its inherent heterogeneity, including domain specific feature spaces and structural diversity across various applications. To address these challenges, we propose a novel structure aware self supervised learning method for Text Attributed Graphs (SSTAG). By leveraging text as a unified representation medium for graph learning, SSTAG bridges the gap between the semantic reasoning of Large Language Models (LLMs) and the structural modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces a dual knowledge distillation framework that co-distills both LLMs and GNNs into structure-aware multilayer perceptrons (MLPs), enhancing the scalability of large-scale TAGs. Additionally, we introduce an in-memory mechanism that stores typical graph representations, aligning them with memory anchors in an in-memory repository to integrate invariant knowledge, thereby improving the model's generalization ability. Extensive experiments demonstrate that SSTAG outperforms state-of-the-art models on cross-domain transfer learning tasks, achieves exceptional scalability, and reduces inference costs while maintaining competitive performance.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.03993",
        "title": "Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning",
        "author": "Yaxin Hou, Bo Han, Yuheng Jia, Hui Liu, Junhui Hou",
        "date": "2025-10-05",
        "abstract": "Current long-tailed semi-supervised learning methods assume that labeled data exhibit a long-tailed distribution, and unlabeled data adhere to a typical predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed). However, the distribution of the unlabeled data is generally unknown and may follow an arbitrary distribution. To tackle this challenge, we propose a Controllable Pseudo-label Generation (CPG) framework, expanding the labeled dataset with the progressively identified reliable pseudo-labels from the unlabeled dataset and training the model on the updated labeled dataset with a known distribution, making it unaffected by the unlabeled data distribution. Specifically, CPG operates through a controllable self-reinforcing optimization cycle: (i) at each training step, our dynamic controllable filtering mechanism selectively incorporates reliable pseudo-labels from the unlabeled dataset into the labeled dataset, ensuring that the updated labeled dataset follows a known distribution; (ii) we then construct a Bayes-optimal classifier using logit adjustment based on the updated labeled data distribution; (iii) this improved classifier subsequently helps identify more reliable pseudo-labels in the next training step. We further theoretically prove that this optimization cycle can significantly reduce the generalization error under some conditions. Additionally, we propose a class-aware adaptive augmentation module to further improve the representation of minority classes, and an auxiliary branch to maximize data utilization by leveraging all labeled and unlabeled samples. Comprehensive evaluations on various commonly used benchmark datasets show that CPG achieves consistent improvements, surpassing state-of-the-art methods by up to \\textbf{15.97\\%} in accuracy. The code is available at https://github.com/yaxinhou/CPG.",
        "journal": "N/A"
    },
    {
        "doi": "10.1007/978-981-97-8798-2_11",
        "title": "An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection",
        "author": "Hamed Fard, Tobias Schalau, Gerhard Wunder",
        "date": "2025-09-27",
        "abstract": "Network intrusion detection, a well-explored cybersecurity field, has predominantly relied on supervised learning algorithms in the past two decades. However, their limitations in detecting only known anomalies prompt the exploration of alternative approaches. Motivated by the success of self-supervised learning in computer vision, there is a rising interest in adapting this paradigm for network intrusion detection. While prior research mainly delved into contrastive self-supervised methods, the efficacy of non-contrastive methods, in conjunction with encoder architectures serving as the representation learning backbone and augmentation strategies that determine what is learned, remains unclear for effective attack detection. This paper compares the performance of five non-contrastive self-supervised learning methods using three encoder architectures and six augmentation strategies. Ninety experiments are systematically conducted on two network intrusion detection datasets, UNSW-NB15 and 5G-NIDD. For each self-supervised model, the combination of encoder architecture and augmentation method yielding the highest average precision, recall, F1-score, and AUCROC is reported. Furthermore, by comparing the best-performing models to two unsupervised baselines, DeepSVDD, and an Autoencoder, we showcase the competitiveness of the non-contrastive methods for attack detection. Code at: https://github.com/renje4z335jh4/non_contrastive_SSL_NIDS",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00190",
        "title": "Automatic Identification of Magnetospheric Regions using Supervised Machine Learning Models",
        "author": "Narges Ahmadi, Robert Ergun, Xiangning Chu, Alex Chasapis, Victoria Wilder",
        "date": "2025-09-30",
        "abstract": "We present an automated approach for identifying magnetospheric regions using supervised machine learning techniques applied to Magnetospheric MultiScale mission data. Our method utilizes ion energy spectra, total magnetic field, total ion temperature, and spacecraft position data to classify five distinct plasma environments: solar wind, magnetosheath, inner magnetosphere, plasma sheet, and lobe regions. The approach combines a convolutional neural network (CNN) for analyzing ion energy spectrogram data with a Random Forest classifier for scalar plasma parameters. The CNN method employs 2D convolution to identify spatial and temporal patterns in the ion energy spectrogram treated as image-like data, while the Random Forest model processes averaged magnetic field, temperature, and position parameters. Our hybrid model achieves 99% accuracy on test dataset with an F1 score of 0.99, providing reliable automated region identification at 3-minute temporal resolution. This lightweight approach requires minimal manual data labeling and can be readily applied to other magnetospheric missions with similar data products.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.01345",
        "title": "Self-Supervised Representation Learning as Mutual Information Maximization",
        "author": "Akhlaqur Rahman Sabby, Yi Sui, Tongzi Wu, Jesse C. Cresswell, Ga Wu",
        "date": "2025-10-01",
        "abstract": "Self-supervised representation learning (SSRL) has demonstrated remarkable empirical success, yet its underlying principles remain insufficiently understood. While recent works attempt to unify SSRL methods by examining their information-theoretic objectives or summarizing their heuristics for preventing representation collapse, architectural elements like the predictor network, stop-gradient operation, and statistical regularizer are often viewed as empirically motivated additions. In this paper, we adopt a first-principles approach and investigate whether the learning objective of an SSRL algorithm dictates its possible optimization strategies and model design choices. In particular, by starting from a variational mutual information (MI) lower bound, we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint MI (JMI), each imposing distinct structural constraints and covering a set of existing SSRL algorithms. SDMI inherently requires alternating optimization, making stop-gradient operations theoretically essential. In contrast, JMI admits joint optimization through symmetric architectures without such components. Under the proposed formulation, predictor networks in SDMI and statistical regularizers in JMI emerge as tractable surrogates for the MI objective. We show that many existing SSRL methods are specific instances or approximations of these two paradigms. This paper provides a theoretical explanation behind the choices of different architectural components of existing SSRL methods, beyond heuristic conveniences.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.04927",
        "title": "Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data",
        "author": "Usman Akram, Yiyue Chen, Haris Vikalo",
        "date": "2025-10-06",
        "abstract": "Training automatic modulation classification (AMC) models on centrally aggregated data raises privacy concerns, incurs communication overhead, and often fails to confer robustness to channel shifts. Federated learning (FL) avoids central aggregation by training on distributed clients but remains sensitive to class imbalance, non-IID client distributions, and limited labeled samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with triplet-loss self-supervision on unlabeled I/Q sequences across clients, followed by per-client SVMs on small labeled sets. We establish convergence of the federated representation learning procedure and a separability guarantee for the downstream classifier under feature noise. Experiments on synthetic and over-the-air datasets show consistent gains over supervised FL baselines under heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.03697",
        "title": "A Time-Bound Signature Scheme for Blockchains",
        "author": "Benjamin Marsh, Paolo Serafino",
        "date": "2025-10-04",
        "abstract": "We introduce a modified Schnorr signature scheme to allow for time-bound signatures for transaction fee auction bidding and smart contract purposes in a blockchain context, ensuring an honest producer can only validate a signature before a given block height. The immutable blockchain is used as a source of universal time for the signature scheme. We show the use of such a signature scheme leads to lower MEV revenue for builders. We then apply our time-bound signatures to Ethereum's EIP-1559 and show how it can be used to mitigate the effect of MEV on predicted equilibrium strategies.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00306",
        "title": "BlockSDN-VC: A SDN-Based Virtual Coordinate-Enhanced Transaction Broadcast Framework for High-Performance Blockchains",
        "author": "Wenyang Jia, Jingjing Wang, Kai Lei",
        "date": "2025-09-30",
        "abstract": "Modern blockchains need fast, reliable propagation to balance security and throughput. Virtual-coordinate methods speed dissemination but rely on slow iterative updates, leaving nodes out of sync. We present BlockSDN-VC, a transaction-broadcast protocol that centralises coordinate computation and forwarding control in an SDN controller, delivering global consistency, minimal path stretch and rapid response to churn or congestion. In geo-distributed simulations, BlockSDN-VC cuts median latency by up to 62% and accelerates convergence fourfold over state-of-the-art schemes with under 3% control-plane overhead. In a real blockchain environment, BlockSDN-VC boosts confirmed-transaction throughput by 17% under adversarial workloads, requiring no modifications to existing clients.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.01740",
        "title": "FOSS-chain: using blockchain for Open Source Software license compliance",
        "author": "Kypros Iacovou, Georgia M. Kapitsaki, Evangelia Vanezi",
        "date": "2025-10-02",
        "abstract": "Open Source Software (OSS) is widely used and carries licenses that indicate the terms under which the software is provided for use, also specifying modification and distribution rules. Ensuring that users are respecting OSS license terms when creating derivative works is a complex process. Compliance issues arising from incompatibilities among licenses may lead to legal disputes. At the same time, the blockchain technology with immutable entries offers a mechanism to provide transparency when it comes to licensing and ensure software changes are recorded. In this work, we are introducing an integration of blockchain and license management when creating derivative works, in order to tackle the issue of OSS license compatibility. We have designed, implemented and performed a preliminary evaluation of FOSS-chain, a web platform that uses blockchain and automates the license compliance process, covering 14 OSS licenses. We have evaluated the initial prototype version of the FOSS-chain platform via a small scale user study. Our preliminary results are promising, demonstrating the potential of the platform for adaptation on realistic software systems.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2509.00480",
        "title": "BPI: A Novel Efficient and Reliable Search Structure for Hybrid Storage Blockchain",
        "author": "Xinkui Zhao, Rengrong Xiong, Guanjie Cheng, Xinhao Jin, Shawn Shi, Xiubo Liang, Gongsheng Yuan, Xiaoye Miao, Jianwei Yin, Shuiguang Deng",
        "date": "2025-08-30",
        "abstract": "Hybrid storage solutions have emerged as potent strategies to alleviate the data storage bottlenecks prevalent in blockchain systems. These solutions harness off-chain Storage Services Providers (SPs) in conjunction with Authenticated Data Structures (ADS) to ensure data integrity and accuracy. Despite these advancements, the reliance on centralized SPs raises concerns about query correctness. Although ADS can verify the existence of individual query results, they fall short of preventing SPs from omitting valid results.   In this paper, we delineate the fundamental distinctions between data search in blockchains and traditional database systems. Drawing upon these insights, we introduce BPI, a lightweight framework that enables efficient keyword queries and maintenance with low overhead. We propose \"Articulated Search\", a query pattern specifically designed for blockchain environments that enhances search efficiency while significantly reducing costs during data user updates. Furthermore, BPI employs a suite of validation models to ensure the inclusion of all valid content in search results while maintaining low overhead.   Extensive experimental evaluations demonstrate that the BPI framework achieves outstanding scalability and performance in keyword searches within blockchain, surpassing EthMB+ and state of the art search databases commonly used in mainstream hybrid storage blockchains (HSB).",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2509.10291",
        "title": "Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case",
        "author": "Salih Toprak, Muge Erel-Ozcevik",
        "date": "2025-09-12",
        "abstract": "In disaster scenarios where conventional energy infrastructure is compromised, secure and traceable energy trading between solar-powered households and mobile charging units becomes a necessity. To ensure the integrity of such transactions over a blockchain network, robust and unpredictable nonce generation is vital. This study proposes an SDN-enabled architecture where machine learning regressors are leveraged not for their accuracy, but for their potential to generate randomized values suitable as nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN allows flexible control over data flows and energy routing policies even in fragmented or degraded networks, ensuring adaptive response during emergencies. Using a 9000-sample dataset, we evaluate five AutoML-selected regression models - Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest Neighbors - not by their prediction accuracy, but by their ability to produce diverse and non-deterministic outputs across shuffled data inputs. Randomness analysis reveals that Random Forest and Extra Trees regressors exhibit complete dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and 99.9%, respectively). These findings highlight that certain machine learning models, particularly tree-based ensembles, may serve as effective and lightweight nonce generators within blockchain-secured, SDN-based energy trading infrastructures resilient to disaster conditions.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2509.09795",
        "title": "Setchain Algorithms for Blockchain Scalability",
        "author": "Arivarasan Karmegam, Gabina Luz Bianchi, Margarita Capretto, Martín Ceresa, Antonio Fernández Anta, César Sánchez",
        "date": "2025-09-11",
        "abstract": "Setchain has been proposed to increase blockchain scalability by relaxing the strict total order requirement among transactions. Setchain organizes elements into a sequence of sets, referred to as epochs, so that elements within each epoch are unordered. In this paper, we propose and evaluate three distinct Setchain algorithms, that leverage an underlying block-based ledger. Vanilla is a basic implementation that serves as a reference point. Compresschain aggregates elements into batches, and compresses these batches before appending them as epochs in the ledger. Hashchain converts batches into fixed-length hashes which are appended as epochs in the ledger. This requires Hashchain to use a distributed service to obtain the batch contents from its hash. To allow light clients to safely interact with only one server, the proposed algorithms maintain, as part of the Setchain, proofs for the epochs. An epoch-proof is the hash of the epoch, cryptographically signed by a server. A client can verify the correctness of an epoch with $f+1$ epoch-proofs (where $f$ is the maximum number of Byzantine servers assumed). All three Setchain algorithms are implemented on top of the CometBFT blockchain application platform. We conducted performance evaluations across various configurations, using clusters of four, seven, and ten servers. Our results show that the Setchain algorithms reach orders of magnitude higher throughput than the underlying blockchain, and achieve finality with latency below 4 seconds.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2509.08267",
        "title": "Exploring Formal Math on the Blockchain: An Explorer for Proofgold",
        "author": "Chad E. Brown, Cezary Kaliszyk, Josef Urban",
        "date": "2025-09-10",
        "abstract": "Proofgold is a blockchain that supports formalized mathematics alongside standard cryptocurrency functionality. It incorporates logical constructs into the blockchain, including declarations of formal theories, definitions, propositions and proofs. It also supports placing and collecting bounties on proving these propositions, incentivizing the development of the formal libraries contained in Proofgold. In this paper, we present a web-based blockchain explorer for Proofgold. The system exposes not only the usual transactional data but also the formal mathematical components embedded in the chain and allows some interaction with them. The explorer allows users to inspect blocks, transactions, and addresses, as well as formal objects: theories, definitions, theorems and their proofs. We also support the submission of transactions to the blockchain using our interface. We describe the system architecture and its integration with the Proofgold Lava software, highlighting how the explorer supports navigation of formal content and facilitates mathematical knowledge management in a decentralized setting, as well as a number of formalizations in category theory done in the system.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2509.15956",
        "title": "Swarm Oracle: Trustless Blockchain Agreements through Robot Swarms",
        "author": "Alexandre Pacheco, Hanqing Zhao, Volker Strobel, Tarik Roukny, Gregory Dudek, Andreagiovanni Reina, Marco Dorigo",
        "date": "2025-09-19",
        "abstract": "Blockchain consensus, rooted in the principle ``don't trust, verify'', limits access to real-world data, which may be ambiguous or inaccessible to some participants. Oracles address this limitation by supplying data to blockchains, but existing solutions may reduce autonomy, transparency, or reintroduce the need for trust. We propose Swarm Oracle: a decentralized network of autonomous robots -- that is, a robot swarm -- that use onboard sensors and peer-to-peer communication to collectively verify real-world data and provide it to smart contracts on public blockchains. Swarm Oracle leverages the built-in decentralization, fault tolerance and mobility of robot swarms, which can flexibly adapt to meet information requests on-demand, even in remote locations. Unlike typical cooperative robot swarms, Swarm Oracle integrates robots from multiple stakeholders, protecting the system from single-party biases but also introducing potential adversarial behavior. To ensure the secure, trustless and global consensus required by blockchains, we employ a Byzantine fault-tolerant protocol that enables robots from different stakeholders to operate together, reaching social agreements of higher quality than the estimates of individual robots. Through extensive experiments using both real and simulated robots, we showcase how consensus on uncertain environmental information can be achieved, despite several types of attacks orchestrated by large proportions of the robots, and how a reputation system based on blockchain tokens lets Swarm Oracle autonomously recover from faults and attacks, a requirement for long-term operation.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2509.06616",
        "title": "Mangrove: Fast and Parallelizable State Replication for Blockchains",
        "author": "Anton Paramonov, Yann Vonlanthen, Quentin Kniep, Jakub Sliwinski, Roger Wattenhofer",
        "date": "2025-09-08",
        "abstract": "Mangrove is a novel scaling approach to building blockchains with parallel smart contract support. Unlike in monolithic blockchains, where a single consensus mechanism determines a strict total order over all transactions, Mangrove uses separate consensus instances per smart contract, without a global order. To allow multiple instances to run in parallel while ensuring that no conflicting transactions are committed, we propose a mechanism called Parallel Optimistic Agreement. Additionally, for simple transactions, we leverage a lightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove is optimized for performance under optimistic conditions, where there is no misbehavior and the network is synchronous. Under these conditions, our protocol can achieve a latency of 2 communication steps between creating and executing a transaction.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2509.11006",
        "title": "A Range-Based Sharding (RBS) Protocol for Scalable Enterprise Blockchain",
        "author": "M. Z. Haider, M. Dias de Assuncao, Kaiwen Zhang",
        "date": "2025-09-13",
        "abstract": "Blockchain technology offers decentralization and security but struggles with scalability, particularly in enterprise settings where efficiency and controlled access are paramount. Sharding is a promising solution for private blockchains, yet existing approaches face challenges in coordinating shards, ensuring fault tolerance with limited nodes, and minimizing the high overhead of consensus mechanisms like PBFT. This paper proposes the Range-Based Sharding (RBS) Protocol, a novel sharding mechanism tailored for enterprise blockchains, implemented on Quorum. Unlike traditional sharding models such as OmniLedger and non-sharding Corda framework, RBS employs a commit-reveal scheme for secure and unbiased shard allocation, ensuring fair validator distribution while reducing cross-shard transaction delays. Our approach enhances scalability by balancing computational loads across shards, reducing consensus overhead, and improving parallel transaction execution. Experimental evaluations demonstrate that RBS achieves significantly higher throughput and lower latency compared to existing enterprise sharding frameworks, making it a viable and efficient solution for largescale blockchain deployments.",
        "journal": "IEEE Conference on Blockchain Computing and Applications (BCCA\n  2025)"
    },
    {
        "doi": "arXiv:2510.01048",
        "title": "Interpreting Language Models Through Concept Descriptions: A Survey",
        "author": "Nils Feldhus, Laura Kopf",
        "date": "2025-10-01",
        "abstract": "Understanding the decision-making processes of neural networks is a central goal of mechanistic interpretability. In the context of Large Language Models (LLMs), this involves uncovering the underlying mechanisms and identifying the roles of individual model components such as neurons and attention heads, as well as model abstractions such as the learned sparse features extracted by Sparse Autoencoders (SAEs). A rapidly growing line of work tackles this challenge by using powerful generator models to produce open-vocabulary, natural language concept descriptions for these components. In this paper, we provide the first survey of the emerging field of concept descriptions for model components and abstractions. We chart the key methods for generating these descriptions, the evolving landscape of automated and human metrics for evaluating them, and the datasets that underpin this research. Our synthesis reveals a growing demand for more rigorous, causal evaluation. By outlining the state of the art and identifying key challenges, this survey provides a roadmap for future research toward making models more transparent.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00690",
        "title": "ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning",
        "author": "Yunhao Wang, Ziting Li, Shuai Chen, Tao Liu, Chao Song, Junjie Jiang, Jian Zhu, Peng Gao, Bin Qin",
        "date": "2025-10-01",
        "abstract": "Aligning large-scale vision-language models (VLMs) for complex reasoning via reinforcement learning is often hampered by the limitations of existing policy optimization algorithms, such as static training schedules and the rigid, uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work, we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework that addresses these challenges through a dual-component adaptive learning strategy. First, ACPO employs a dynamic curriculum that orchestrates a principled transition from a stable, near on-policy exploration phase to an efficient, off-policy exploitation phase by progressively increasing sample reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism that replaces the fixed clipping hyperparameter with dynamic, sample-wise bounds modulated by the normalized advantage of each token. This allows for more granular and robust policy updates, enabling larger gradients for high-potential samples while safeguarding against destructive ones. We conduct extensive experiments on a suite of challenging multimodal reasoning benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate that ACPO consistently outperforms strong baselines such as DAPO and PAPO, achieving state-of-the-art performance, accelerated convergence, and superior training stability.",
        "journal": "N/A"
    },
    {
        "doi": "10.1145/3700410.3702114",
        "title": "Reference-free automatic speech severity evaluation using acoustic unit language modelling",
        "author": "Bence Mark Halpern, Tomoki Toda",
        "date": "2025-10-01",
        "abstract": "Speech severity evaluation is becoming increasingly important as the economic burden of speech disorders grows. Current speech severity models often struggle with generalization, learning dataset-specific acoustic cues rather than meaningful correlates of speech severity. Furthermore, many models require reference speech or a transcript, limiting their applicability in ecologically valid scenarios, such as spontaneous speech evaluation. Previous research indicated that automatic speech naturalness evaluation scores correlate strongly with severity evaluation scores, leading us to explore a reference-free method, SpeechLMScore, which does not rely on pathological speech data. Additionally, we present the NKI-SpeechRT dataset, based on the NKI-CCRT dataset, to provide a more comprehensive foundation for speech severity evaluation. This study evaluates whether SpeechLMScore outperforms traditional acoustic feature-based approaches and assesses the performance gap between reference-free and reference-based models. Moreover, we examine the impact of noise on these models by utilizing subjective noise ratings in the NKI-SpeechRT dataset. The results demonstrate that SpeechLMScore is robust to noise and offers superior performance compared to traditional approaches.",
        "journal": "In Proceedings of the 6th ACM International Conference on\n  Multimedia in Asia Workshops (pp. 1-5) (2024)"
    },
    {
        "doi": "arXiv:2510.00446",
        "title": "LongCodeZip: Compress Long Context for Code Language Models",
        "author": "Yuling Shi, Yichun Qian, Hongyu Zhang, Beijun Shen, Xiaodong Gu",
        "date": "2025-10-01",
        "abstract": "Code generation under long contexts is becoming increasingly critical as Large Language Models (LLMs) are required to reason over extensive information in the codebase. While recent advances enable code LLMs to process long inputs, high API costs and generation latency remain substantial bottlenecks. Existing context pruning techniques, such as LLMLingua, achieve promising results for general text but overlook code-specific structures and dependencies, leading to suboptimal performance in programming tasks. In this paper, we propose LongCodeZip, a novel plug-and-play code compression framework designed specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1) coarse-grained compression, which identifies and ranks function-level chunks using conditional perplexity with respect to the instruction, retaining only the most relevant functions; and (2) fine-grained compression, which segments retained functions into blocks based on perplexity and selects an optimal subset under an adaptive token budget to maximize relevance. Evaluations across multiple tasks, including code completion, summarization, and question answering, show that LongCodeZip consistently outperforms baseline methods, achieving up to a 5.6x compression ratio without degrading task performance. By effectively reducing context size while preserving essential information, LongCodeZip enables LLMs to better scale to real-world, large-scale code scenarios, advancing the efficiency and capability of code intelligence applications.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00373",
        "title": "Combining Large Language Models and Gradient-Free Optimization for Automatic Control Policy Synthesis",
        "author": "Carlo Bosio, Matteo Guarrera, Alberto Sangiovanni-Vincentelli, Mark W. Mueller",
        "date": "2025-10-01",
        "abstract": "Large Language models (LLMs) have shown promise as generators of symbolic control policies, producing interpretable program-like representations through iterative search. However, these models are not capable of separating the functional structure of a policy from the numerical values it is parametrized by, thus making the search process slow and inefficient. We propose a hybrid approach that decouples structural synthesis from parameter optimization by introducing an additional optimization layer for local parameter search. In our method, the numerical parameters of LLM-generated programs are extracted and optimized numerically to maximize task performance. With this integration, an LLM iterates over the functional structure of programs, while a separate optimization loop is used to find a locally optimal set of parameters accompanying candidate programs. We evaluate our method on a set of control tasks, showing that it achieves higher returns and improved sample efficiency compared to purely LLM-guided search. We show that combining symbolic program synthesis with numerical optimization yields interpretable yet high-performing policies, bridging the gap between language-model-guided design and classical control tuning. Our code is available at https://sites.google.com/berkeley.edu/colmo.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00294",
        "title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models",
        "author": "Shutong Wu, Jiawei Zhang",
        "date": "2025-09-30",
        "abstract": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges like the famous \"reversal curse\" or learning under data-constrained scenarios. However, this bidirectional nature also brings an obstacle that DLLMs are not inherently compatible with KV Cache, and consequently, the inference efficiency is not competitive compared with autoregressive models. Taking advantage of their inherent capability of multi-token prediction, existing parallel decoding algorithms can speed up the DLLM inference, but at the cost of non-negligible performance degradation. To overcome this challenge, we introduce Free Draft-and-Verification (Freedave), a novel fast sampling algorithm tailored for DLLMs that achieves lossless parallel decoding. Specifically, we propose a pipeline of parallel-decoded candidate generation and verification, which is guaranteed to reproduce the same sequence generated by static sampling, without introducing extra model forward calls. By applying Freedave, the throughput of DLLMs can be boosted up to $2.8\\times$ without performance degradation on math reasoning tasks.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.00125",
        "title": "Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning",
        "author": "Hong kyu Lee, Ruixuan Liu, Li Xiong",
        "date": "2025-09-30",
        "abstract": "Machine unlearning is an emerging technique that removes the influence of a subset of training data (forget set) from a model without full retraining, with applications including privacy protection, content moderation, and model correction. The key challenge lies in ensuring that the model completely forgets the knowledge of the forget set without compromising its overall utility. Existing unlearning methods for large language models (LLMs) often utilize auxiliary language models, retain datasets, or even commercial AI services for effective unlearning and maintaining the model utility. However, dependence on these external resources is often impractical and could potentially introduce additional privacy risks. In this work, we propose direct token optimization (DTO), a novel self-contained unlearning approach for LLMs that directly optimizes the token level objectives and eliminates the need for external resources. Given a sequence to unlearn, we identify two categories of tokens: target tokens, which capture critical knowledge for unlearning, and the remaining non-target tokens, which are crucial for maintaining the model utility. The former are used to optimize the unlearning objective, while the latter serve to preserve the model's performance. The experimental results show that the proposed DTO achieves up to 16.8$\\times$ improvement in forget quality on several benchmark datasets than the latest baselines while maintaining a comparable level of model utility.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.01246",
        "title": "A Comparative Analysis of Sparse Autoencoder and Activation Difference in Language Model Steering",
        "author": "Jiaqing Xie",
        "date": "2025-09-24",
        "abstract": "Sparse autoencoders (SAEs) have recently emerged as a powerful tool for language model steering. Prior work has explored top-k SAE latents for steering, but we observe that many dimensions among the top-k latents capture non-semantic features such as punctuation rather than semantic attributes like instructions. To address this, we propose focusing on a single, most relevant SAE latent (top-1), eliminating redundant features. We further identify a limitation in constant SAE steering, which often produces degenerate outputs such as repetitive single words. To mitigate this, we introduce a token-wise decaying steering strategy, enabling more faithful comparisons with mean activation difference baselines. Empirically, we show that steering an SAE latent associated with reasoning reliably elicits step-by-step mathematical reasoning and enhances inference quality, functionally resembling the effect of appending a guiding token. Our results demonstrate that SAEs outperform mean activation difference methods on mathematical reasoning benchmarks and match their performance on IF-Eval.",
        "journal": "N/A"
    },
    {
        "doi": "arXiv:2510.01244",
        "title": "Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model",
        "author": "Hyeoneui Kim, Jeongha Kim, Huijing Xu, Jinsun Jung, Sunghoon Kang, Sun Joo Jang",
        "date": "2025-09-24",
        "abstract": "Stress, arising from the dynamic interaction between external stressors, individual appraisals, and physiological or psychological responses, significantly impacts health yet is often underreported and inconsistently documented, typically captured as unstructured free-text in electronic health records. Ambient AI technologies offer promise in reducing documentation burden, but predominantly generate unstructured narratives, limiting downstream clinical utility.   This study aimed to develop an ontology for mental stress and evaluate the feasibility of using a Large Language Model (LLM) to extract ontology-guided stress-related information from narrative text. The Mental Stress Ontology (MeSO) was developed by integrating theoretical models like the Transactional Model of Stress with concepts from 11 validated stress assessment tools. MeSO's structure and content were refined using Ontology Pitfall Scanner! and expert validation.   Using MeSO, six categories of stress-related information--stressor, stress response, coping strategy, duration, onset, and temporal profile--were extracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated accuracy and ontology coverage. The final ontology included 181 concepts across eight top-level classes. Of 220 extractable stress-related items, the LLM correctly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21 (9.5%). All correctly extracted items were accurately mapped to MeSO, although 24 relevant concepts were not yet represented in the ontology.   This study demonstrates the feasibility of using an ontology-guided LLM for structured extraction of stress-related information, offering potential to enhance the consistency and utility of stress documentation in ambient AI systems. Future work should involve clinical dialogue data and comparison across LLMs.",
        "journal": "N/A"
    },
    {
        "doi": "10.1109/SUMMA64428.2024.10803746",
        "title": "Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation",
        "author": "Andrei Lazarev, Dmitrii Sedov",
        "date": "2025-09-22",
        "abstract": "The exponential growth of information presents a significant challenge for researchers and professionals seeking to remain at the forefront of their fields and this paper introduces an innovative framework for automatically generating insightful financial digests using the power of Large Language Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of data extraction from OpenAlex, strategic prompt engineering, and LLM-driven analysis, we demonstrate the automated example of creating a comprehensive digests that generalize key findings, identify emerging trends. This approach addresses the limitations of traditional analysis methods, enabling the efficient processing of vast amounts of unstructured data and the delivery of actionable insights in an easily digestible format. This paper describes how LLMs work in simple words and how we can use their power to help researchers and scholars save their time and stay informed about current trends. Our study includes step-by-step process, from data acquisition and JSON construction to interaction with Gemini and the automated generation of PDF reports, including a link to the project's GitHub repository for broader accessibility and further development.",
        "journal": "2024 6th International Conference on Control Systems, Mathematical\n  Modeling, Automation and Energy Efficiency (SUMMA), Lipetsk, Russian\n  Federation, 2024, pp. 317-321"
    }
]